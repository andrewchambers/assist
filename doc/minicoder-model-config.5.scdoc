minicoder-model-config(5)

# NAME

minicoder-model-config - AI model configuration file format for minicoder(1)

# SYNOPSIS

$MINICODER_MODEL_CONFIG

# DESCRIPTION

The *minicoder* command uses JSON configuration files to define available AI models and their parameters. It supports multiple model providers through OpenAI-compatible APIs, including OpenAI, Anthropic (via proxy), Google, local models via Ollama, and many others through services like OpenRouter.

If the *$MINICODER_MODEL_CONFIG* environment variable is set, it specifies the path to a custom model configuration file. Otherwise, built-in defaults are used based on available API keys.

The first model defined in the configuration file becomes the default model when no *--model* option is specified.

# FILE FORMAT

The configuration file must contain a JSON object where each key is a model name and the value is a model definition object.

## Model Definition

Each model definition must include:

*type* (string, required)
	The model type. Currently only "openai" is supported, which works with any OpenAI-compatible API including OpenAI, Ollama, OpenRouter, and many other providers.

*endpoint* (string, required for openai type)
	The API endpoint URL for the model.

*model* (string, optional)
	The specific model identifier to use in API requests.

*api_key* (string, optional)
	The API key for authentication. Can be specified directly or via *api_key_env*.

*api_key_env* (string, optional)
	Environment variable name containing the API key. Takes precedence over *api_key*.

*max_context_bytes* (number, optional)
	Maximum context size in bytes (approximate). Default: 512000 (~128K tokens).

*params* (object, optional)
	Additional parameters to send with API requests. Must be a valid JSON object.

## Parameter Details

## OpenAI Type Parameters

For models of type "openai", the following parameters in the *params* object are commonly used:

*stream* (boolean)
	Enable streaming responses. Default: true.

*reasoning* (object)
	For models that support reasoning (e.g., o3, gemini):
	- *effort*: "low", "medium", or "high"

*temperature* (number)
	Sampling temperature (0.0 to 2.0).

*max_tokens* (number)
	Maximum tokens to generate.

# EXAMPLES

Basic configuration with two models:

```
{
  "gpt4": {
    "type": "openai",
    "endpoint": "https://api.openai.com/v1/chat/completions",
    "model": "gpt-4-turbo",
    "api_key_env": "OPENAI_API_KEY",
    "max_context_bytes": 512000,
    "params": {
      "stream": true,
      "temperature": 0.7
    }
  },
  "local-llama": {
    "type": "openai",
    "endpoint": "http://localhost:8080/v1/chat/completions",
    "model": "llama-3.1-8b",
    "max_context_bytes": 32768,
    "params": {
      "stream": false
    }
  }
}
```

Configuration using OpenRouter with reasoning models:

```
{
  "o3": {
    "type": "openai", 
    "endpoint": "https://openrouter.ai/api/v1/chat/completions",
    "model": "openai/o3",
    "api_key_env": "OPENROUTER_API_KEY",
    "max_context_bytes": 512000,
    "params": {
      "reasoning": {
        "effort": "high"
      },
      "stream": true
    }
  },
  "gemini": {
    "type": "openai",
    "endpoint": "https://openrouter.ai/api/v1/chat/completions", 
    "model": "google/gemini-2.5-pro",
    "api_key_env": "OPENROUTER_API_KEY",
    "max_context_bytes": 2097152,
    "params": {
      "reasoning": {
        "effort": "high"
      },
      "stream": true
    }
  }
}
```

Configuration for Ollama (local model server):

```
{
  "ollama-llama": {
    "type": "openai",
    "endpoint": "http://localhost:11434/v1/chat/completions",
    "model": "llama3.1:latest",
    "max_context_bytes": 32768,
    "params": {
      "stream": true,
      "temperature": 0.7
    }
  },
  "ollama-mistral": {
    "type": "openai",
    "endpoint": "http://localhost:11434/v1/chat/completions",
    "model": "mistral:latest",
    "max_context_bytes": 32768,
    "params": {
      "stream": true
    }
  }
}
```

# DEFAULT MODELS

If no configuration file is found, *minicoder* uses built-in models based on available API keys:

- With *OPENROUTER_API_KEY*: Provides access to o3, o3-pro, o4-mini, gemini, grok-4, deepseek-r1, glm-4.5, sonnet, and opus models
- With *OPENAI_API_KEY*: Provides access to o3 and o4-mini models directly from OpenAI
- With *GEMINI_API_KEY*: Provides access to gemini model directly from Google
- With *XAI_API_KEY*: Provides access to grok-4 model directly from X.AI
- Local models: local/qwen3-30b (via Ollama on localhost:11434)

When multiple API keys are set, OpenRouter is preferred as it provides unified access to multiple providers. The first model in the list becomes the default when no --model option is specified.

# ENVIRONMENT VARIABLES

*MINICODER_MODEL_CONFIG*
	Path to a custom model configuration file. If not set, built-in defaults are used.

API keys can be specified via environment variables using the *api_key_env* field. Common variables:

*OPENAI_API_KEY*
	For OpenAI API access (used for OpenAI models like o3, o4-mini)

*OPENROUTER_API_KEY*
	For OpenRouter service access (provides access to models from multiple providers)

*GEMINI_API_KEY*
	For Google Gemini API access

*XAI_API_KEY*
	For X.AI API access (used for Grok models)

# SEE ALSO

*minicoder*(1)

# AUTHORS

Maintained by the assist project contributors.